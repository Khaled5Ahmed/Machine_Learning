{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c329bf4-99c5-4e76-8ab1-b5789d7b4986",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c73fbc04-2ee4-481c-adeb-7e9cb9cdca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c228971-88ed-4531-b4a3-8c4b81605e73",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c35afcb8-c39c-45c2-b201-389cb806b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(X, y, random_state=42):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.30, stratify=y, random_state=random_state\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "    return (X_train.reset_index(drop=True), y_train.reset_index(drop=True)), \\\n",
    "           (X_val.reset_index(drop=True), y_val.reset_index(drop=True)), \\\n",
    "           (X_test.reset_index(drop=True), y_test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310b0b5-e2cf-4c34-a41f-bddce419d073",
   "metadata": {},
   "source": [
    "# Core functions for Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c34a949b-f8c2-4340-831f-b1275e6122a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    if len(labels) == 0: return 0.0\n",
    "    counts = Counter(labels)\n",
    "    n = len(labels)\n",
    "    ent = 0.0\n",
    "    for count in counts.values():\n",
    "        p = count / n\n",
    "        if p > 0:\n",
    "            ent -= p * np.log2(p)\n",
    "    return ent\n",
    "\n",
    "def information_gain(parent_labels, splits):\n",
    "    H_parent = entropy(parent_labels)\n",
    "    n = len(parent_labels)\n",
    "    weighted_child_entropy = 0.0\n",
    "    for child_labels in splits.values():\n",
    "        if len(child_labels) > 0:\n",
    "            weighted_child_entropy += (len(child_labels)/n) * entropy(child_labels)\n",
    "    return H_parent - weighted_child_entropy\n",
    "\n",
    "def find_best_split_continuous(X_col, y, min_samples_leaf):\n",
    "    data = pd.DataFrame({'feature': X_col, 'label': y}).sort_values('feature')\n",
    "    unique_values = data['feature'].unique()\n",
    "    if len(unique_values)<2: return None, None, None\n",
    "    candidate_thresholds = (unique_values[:-1]+unique_values[1:])/2\n",
    "    best_gain, best_threshold, best_splits = -1, None, None\n",
    "    n = len(data)\n",
    "    for threshold in candidate_thresholds:\n",
    "        left_mask = data['feature'] <= threshold\n",
    "        y_left = data['label'].loc[left_mask.index[left_mask]]\n",
    "        y_right = data['label'].loc[left_mask.index[~left_mask]]\n",
    "        if len(y_left)<min_samples_leaf or len(y_right)<min_samples_leaf: continue\n",
    "        splits = {f'<={threshold:.4f}': y_left, f'>{threshold:.4f}': y_right}\n",
    "        gain = information_gain(y, splits)\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_threshold, best_splits = gain, threshold, splits\n",
    "    return best_threshold, best_gain, best_splits\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Node & DecisionTreeIG\n",
    "# ----------------------------\n",
    "class Node:\n",
    "    def __init__(self, depth=0):\n",
    "        self.depth = depth\n",
    "        self.is_leaf = False\n",
    "        self.prediction = None\n",
    "        self.feature = None\n",
    "        self.split_value = None\n",
    "        self.children = {}\n",
    "        self.n_samples = 0\n",
    "        self.n_classes = {}\n",
    "        self.entropy = 0.0\n",
    "        self.is_continuous_split = False\n",
    "\n",
    "class DecisionTreeIG:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.root = None\n",
    "        self.feature_importance_ = defaultdict(float)\n",
    "        self.n_nodes_ = 0\n",
    "        self.max_observed_depth_ = 0\n",
    "        self.feature_gains_ = defaultdict(list)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.feature_types = {col:X[col].dtype for col in X.columns}\n",
    "        self.root = self._build_tree(X, y, list(X.columns), 0)\n",
    "        total_importance = sum(self.feature_importance_.values())\n",
    "        if total_importance>0:\n",
    "            for feat in self.feature_importance_: self.feature_importance_[feat]/=total_importance\n",
    "        return self\n",
    "\n",
    "    def _majority_class(self, labels):\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "    def _build_tree(self, X, y, available_features, depth):\n",
    "        node = Node(depth=depth)\n",
    "        node.n_samples = len(y)\n",
    "        node.n_classes = dict(Counter(y))\n",
    "        node.entropy = entropy(y)\n",
    "        self.n_nodes_ += 1\n",
    "        self.max_observed_depth_ = max(self.max_observed_depth_, depth)\n",
    "        if len(set(y))==1 or not available_features or (self.max_depth and depth>=self.max_depth) or len(y)<self.min_samples_split:\n",
    "            node.is_leaf = True\n",
    "            node.prediction = self._majority_class(y)\n",
    "            return node\n",
    "        best_feature, best_gain, best_splits, best_split_value, is_continuous = None, -1, None, None, False\n",
    "        for feature in available_features:\n",
    "            f_dtype = self.feature_types[feature]\n",
    "            if f_dtype in ['float64','int64']:\n",
    "                threshold, gain, splits = find_best_split_continuous(X[feature], y, self.min_samples_leaf)\n",
    "                if gain is not None and gain>best_gain:\n",
    "                    best_feature, best_gain, best_splits, best_split_value, is_continuous = feature, gain, splits, threshold, True\n",
    "            else:\n",
    "                groups = {v:y.loc[idxs] for v, idxs in X.groupby(feature).groups.items()}\n",
    "                if any(len(lbls)<self.min_samples_leaf for lbls in groups.values()): continue\n",
    "                gain = information_gain(y, groups)\n",
    "                if gain>best_gain: best_feature, best_gain, best_splits, best_split_value, is_continuous = feature, gain, groups, None, False\n",
    "        if best_feature is None or best_gain<=1e-12:\n",
    "            node.is_leaf = True\n",
    "            node.prediction = self._majority_class(y)\n",
    "            return node\n",
    "        self.feature_importance_[best_feature] += best_gain\n",
    "        self.feature_gains_[best_feature].append(best_gain)\n",
    "        node.feature = best_feature\n",
    "        node.split_value = best_split_value\n",
    "        node.is_continuous_split = is_continuous\n",
    "        remaining_features = available_features.copy()\n",
    "        if is_continuous:\n",
    "            threshold = best_split_value\n",
    "            mask_left = X[best_feature]<=threshold\n",
    "            node.children[f'<={threshold:.4f}'] = self._build_tree(X.loc[mask_left,remaining_features], y.loc[mask_left], remaining_features, depth+1)\n",
    "            node.children[f'>{threshold:.4f}'] = self._build_tree(X.loc[~mask_left,remaining_features], y.loc[~mask_left], remaining_features, depth+1)\n",
    "        else:\n",
    "            for value, lbls in best_splits.items():\n",
    "                mask = X[best_feature]==value\n",
    "                node.children[value] = self._build_tree(X.loc[mask,remaining_features], y.loc[mask], remaining_features, depth+1)\n",
    "        node.prediction = self._majority_class(y)\n",
    "        return node\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            val = x.get(node.feature, 'UNKNOWN')\n",
    "            if node.is_continuous_split:\n",
    "                if val=='UNKNOWN': return node.prediction\n",
    "                branch = f'<={node.split_value:.4f}' if val<=node.split_value else f'>{node.split_value:.4f}'\n",
    "                node = node.children.get(branch, Node()) \n",
    "            else:\n",
    "                node = node.children.get(val, Node())\n",
    "            if node is None: return None\n",
    "        return node.prediction\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_one(X.iloc[i].to_dict()) for i in range(len(X))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599824f-c634-4c9c-aead-98f8c228e4d4",
   "metadata": {},
   "source": [
    "# DecisionTreeIG subclass for feature subspace (for Random Forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3eaf6ed-c858-4834-95ab-313db263ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeIG_Subspace(DecisionTreeIG):\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                 max_features_at_split=None, random_state=None):\n",
    "        super().__init__(max_depth, min_samples_split, min_samples_leaf)\n",
    "        self.max_features_at_split = max_features_at_split\n",
    "        if random_state: random.seed(random_state); np.random.seed(random_state)\n",
    "\n",
    "    def _build_tree(self, X, y, available_features, depth):\n",
    "        if self.max_features_at_split is not None and len(available_features)>self.max_features_at_split:\n",
    "            available_features = random.sample(available_features, self.max_features_at_split)\n",
    "        return super()._build_tree(X, y, available_features, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b9dcf-0735-431a-b18b-adcd04c81955",
   "metadata": {},
   "source": [
    "# Random Forest class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7fc3b4b-935e-45ff-8e76-c4ba1d6d1c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest_Subspace:\n",
    "    def __init__(self, n_estimators=10, max_features=None, max_depth=None,\n",
    "                 min_samples_split=2, min_samples_leaf=1, random_state=None, verbose=False):\n",
    "        self.n_estimators, self.max_features = n_estimators, max_features\n",
    "        self.max_depth, self.min_samples_split, self.min_samples_leaf = max_depth, min_samples_split, min_samples_leaf\n",
    "        self.random_state, self.trees, self.verbose = random_state, [], verbose\n",
    "        if random_state: random.seed(random_state); np.random.seed(random_state)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        n = len(X)\n",
    "        self.trees=[]\n",
    "        for i in range(self.n_estimators):\n",
    "            idxs = np.random.choice(n,n,replace=True)\n",
    "            Xb, yb = X.iloc[idxs].reset_index(drop=True), y.iloc[idxs].reset_index(drop=True)\n",
    "            tree = DecisionTreeIG_Subspace(max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                           min_samples_leaf=self.min_samples_leaf, max_features_at_split=self.max_features,\n",
    "                                           random_state=None if self.random_state is None else self.random_state+i)\n",
    "            tree.fit(Xb,yb)\n",
    "            self.trees.append(tree)\n",
    "            if self.verbose: print(f\"Tree {i+1}/{self.n_estimators} trained, nodes={tree.n_nodes_}\")\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.array([Counter(row).most_common(1)[0][0] for row in preds.T])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb227f-3f83-4aad-9357-b8a7ceb2b5be",
   "metadata": {},
   "source": [
    "# Load dataset & split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10f7fabb-210b-442e-99ff-de6da69cd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame.copy()\n",
    "df['class'] = df['target'].map({0:'malignant',1:'benign'})\n",
    "df = df.drop(columns=['target'])\n",
    "X_full = df.drop(columns=['class'])\n",
    "y_full = df['class']\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = create_split(X_full, y_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2c5fc-8238-4eed-a0ba-e20a3e200852",
   "metadata": {},
   "source": [
    "# Random Forest tuning & evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f09699ae-b3d6-4d02-90dc-8633f2f16206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top tuning results:\n",
      "     T  max_features   val_acc\n",
      "0  10            15  0.964706\n",
      "1  30            15  0.952941\n",
      "2  30             5  0.941176\n",
      "3  50            15  0.941176\n",
      "4   5             5  0.929412\n",
      "5   5            15  0.929412\n",
      "6  50             5  0.929412\n",
      "7  10             5  0.917647\n",
      "\n",
      "Best config: T=10, max_features=15 -> val_acc=0.9647\n"
     ]
    }
   ],
   "source": [
    "d = X_train.shape[1]\n",
    "mf_options = sorted(list(set([int(math.floor(math.sqrt(d))), int(math.floor(d/2))])))\n",
    "T_values = [5,10,30,50]\n",
    "depth_choice, min_split_choice = 6, 2\n",
    "\n",
    "best_rf, best_cfg, best_acc = None, None, -1\n",
    "results=[]\n",
    "for T in T_values:\n",
    "    for mf in mf_options:\n",
    "        rf = RandomForest_Subspace(n_estimators=T, max_features=mf, max_depth=depth_choice,\n",
    "                                   min_samples_split=min_split_choice, min_samples_leaf=1, verbose=False)\n",
    "        rf.fit(X_train,y_train)\n",
    "        val_acc = accuracy_score(y_val, rf.predict(X_val))\n",
    "        results.append({'T':T,'max_features':mf,'val_acc':val_acc})\n",
    "        if val_acc>best_acc: best_acc,best_cfg,best_rf=val_acc,(T,mf),rf\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('val_acc',ascending=False).reset_index(drop=True)\n",
    "print(\"\\nTop tuning results:\\n\", results_df.head(10))\n",
    "print(f\"\\nBest config: T={best_cfg[0]}, max_features={best_cfg[1]} -> val_acc={best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392deba1-dd85-4f1b-969b-0b82094e12be",
   "metadata": {},
   "source": [
    "# Retrain best RF on train+val, evaluate test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d1ac23e-7240-4934-8818-4c946052f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1/10 trained, nodes=23\n",
      "Tree 2/10 trained, nodes=25\n",
      "Tree 3/10 trained, nodes=27\n",
      "Tree 4/10 trained, nodes=23\n",
      "Tree 5/10 trained, nodes=25\n",
      "Tree 6/10 trained, nodes=27\n",
      "Tree 7/10 trained, nodes=23\n",
      "Tree 8/10 trained, nodes=29\n",
      "Tree 9/10 trained, nodes=21\n",
      "Tree 10/10 trained, nodes=23\n",
      "\n",
      "Random Forest Test Accuracy: 0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9643    1.0000    0.9818        54\n",
      "   malignant     1.0000    0.9375    0.9677        32\n",
      "\n",
      "    accuracy                         0.9767        86\n",
      "   macro avg     0.9821    0.9688    0.9748        86\n",
      "weighted avg     0.9776    0.9767    0.9766        86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_val = pd.concat([X_train,X_val],axis=0).reset_index(drop=True)\n",
    "y_train_val = pd.concat([y_train,y_val],axis=0).reset_index(drop=True)\n",
    "best_T, best_mf = best_cfg\n",
    "rf_final = RandomForest_Subspace(n_estimators=best_T,max_features=best_mf,\n",
    "                                 max_depth=depth_choice,min_samples_split=min_split_choice,\n",
    "                                 random_state=42,verbose=True)\n",
    "rf_final.fit(X_train_val,y_train_val)\n",
    "y_test_pred_rf = rf_final.predict(X_test)\n",
    "rf_test_acc = accuracy_score(y_test,y_test_pred_rf)\n",
    "print(f\"\\nRandom Forest Test Accuracy: {rf_test_acc:.4f}\")\n",
    "print(classification_report(y_test,y_test_pred_rf,digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
